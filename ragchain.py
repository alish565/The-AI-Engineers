# -*- coding: utf-8 -*-
"""RagChain_with_image_captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19auroyi3cE2fZXJk5EFEuHQ_xXYVc7BY
"""

!pip install pymupdf pillow pytesseract
!apt install -y libomp-dev
!pip install faiss-cpu
!pip install langchain-openai
!pip install --upgrade langchain langchain-core langchain-community

import os
from langchain_community.document_loaders import PyMuPDFLoader, ImageCaptionLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from google.colab import userdata
from langchain.prompts import PromptTemplate
from urllib.request import urlretrieve
from langchain.chains import RetrievalQA

#i wrote the code in google colab, here iam setting the openai api key  inorder to use openai llm
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

url="https://institute.aljazeera.net/sites/default/files/2018/mobile%20journalisn%20english.pdf"
#creating the directorty where the pdf is going to be stored after loading it from the url
os.makedirs("mobile_journalism", exist_ok=True)

file_path = os.path.join("mobile_journalism", url.rpartition("/")[2])
urlretrieve(url, file_path)

#loading the pdf
loader = PyMuPDFLoader(file_path)
docs = loader.load()

#splitting the pdf into chuncks of 500 with 100 overlap
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunked_docs = splitter.split_documents(docs)

#for the embedding i sed openai
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
# used faiss f as the vectorestore, and stored the docs with the embedding
faiss_index = FAISS.from_documents(chunked_docs, embeddings)

#the llm used for the project is gpt-3.5 model
llm = ChatOpenAI(model="gpt-3.5-turbo")

#setting faiss index as retriever
retriever = faiss_index.as_retriever()

#creating the promt template guiding the model that he should response from the context provided
prompt_template = """Use the following pieces of context to answer the question at the end. Please follow the following rules:
1. If you don't know the answer, don't try to make up an answer. Just say "I can't find the final answer but you may want to check the following links".
2. If you find the answer, write the answer in a concise way with five sentences maximum.

{context}

Question: {question}

Helpful Answer:
"""

PROMPT = PromptTemplate(
 template=prompt_template, input_variables=["context", "question"]
)

#creating the RAG chain
retrievalQA = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs = {"prompt" : PROMPT}
)

query="what applications are used for mobile journalism"
result = retrievalQA.invoke({"query" : query})
print(result['result'])
