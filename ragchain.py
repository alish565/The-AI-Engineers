# -*- coding: utf-8 -*-
"""RagChain_with_image_captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19auroyi3cE2fZXJk5EFEuHQ_xXYVc7BY
"""

!pip install pymupdf pillow pytesseract
!apt install -y libomp-dev
!pip install faiss-cpu
!pip install langchain-openai
!pip install --upgrade langchain langchain-core langchain-community

import os
from langchain_community.document_loaders import PyMuPDFLoader, ImageCaptionLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from google.colab import userdata
from langchain.prompts import PromptTemplate
from urllib.request import urlretrieve
from langchain.chains import RetrievalQA

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

url="https://institute.aljazeera.net/sites/default/files/2018/mobile%20journalisn%20english.pdf"

os.makedirs("mobile_journalism", exist_ok=True)

file_path = os.path.join("mobile_journalism", url.rpartition("/")[2])
urlretrieve(url, file_path)

loader = PyMuPDFLoader(file_path)
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunked_docs = splitter.split_documents(docs)


embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
faiss_index = FAISS.from_documents(chunked_docs, embeddings)

llm = ChatOpenAI(model="gpt-3.5-turbo")

retriever = faiss_index.as_retriever()

prompt_template = """Use the following pieces of context to answer the question at the end. Please follow the following rules:
1. If you don't know the answer, don't try to make up an answer. Just say "I can't find the final answer but you may want to check the following links".
2. If you find the answer, write the answer in a concise way with five sentences maximum.

{context}

Question: {question}

Helpful Answer:
"""

PROMPT = PromptTemplate(
 template=prompt_template, input_variables=["context", "question"]
)

retrievalQA = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs = {"prompt" : PROMPT}
)

query="what applications are used for mobile journalism"
result = retrievalQA.invoke({"query" : query})
print(result['result'])